import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

# 1. Define the corpus
corpus = [
    'I love pizza',
    'I love burgers',
    'You love pizza',
    'She likes pasta',
    'He likes burgers'
]

# 2. Tokenize words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1  # +1 for padding
print('Vocabulary:', word_index)

# 3. Prepare dataset (state -> action pairs)
states = []
actions = []

for line in corpus:
    tokens = tokenizer.texts_to_sequences([line])[0]
    for i in range(len(tokens) - 1):
        states.append(tokens[i])
        actions.append(tokens[i + 1])

states = np.array(states)
actions = np.array(actions)

print('States:', states)
print('Actions:', actions)

# 4. Build a simple model
model = Sequential([
    Embedding(vocab_size, 10, input_length=1),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(vocab_size, activation='softmax')
])

# 5. Compile
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 6. Train (small epochs!)
model.fit(states, actions, epochs=30, verbose=2)

# 7. Test simple prediction
def predict_next_word(word):
    token = tokenizer.texts_to_sequences([word])[0]
    if not token:
        return "Word not found."
    prediction = model.predict(np.array([token[0]]))
    predicted_index = np.argmax(prediction)

    for w, i in word_index.items():
        if i == predicted_index:
            return w
    return "?"

# 8. Try it
test_word = 'likes'
next_word = predict_next_word(test_word)
print(f"\nGiven '{test_word}', predicted next word is '{next_word}'")
